<html>
<head>
  <title>CSCI 150 - Lab 9 - Gibberish Generator</title>
  <link rel="stylesheet" type="text/css" title="Default" href="../style.css">
</head>

<body>
<h1><a href="../index.html">CSCI 150</a> - Lab 9<br>Gibberish Generator</h1>
<hr>

<h2>Materials</h2>
<ul>
<li>Pythnon <a href="https://docs.python.org/2.7/library/stdtypes.html#mapping-types-dict">dict</a> object.
<li><a href="../data/english2.txt">English Word List</a>
<li><a href="../data/latin2.txt">Latin Word List</a>
<li><a href="../data/spanish2.txt">Spanish Word List</a>
</ul>

<h2>Overview</h2>
<blockquote>
'Twas brillig, and the slithy toves<br>
Did gyre and gimble in the wabe;<br>
All mimsy were the borogoves,<br>
And the mome raths outgrabe.<br>
</blockquote>
This assignment explores the use of the dictionary object to generate pronounceable nonsense words,
known as <a href="http://en.wikipedia.org/wiki/Pseudoword">pseudowords</a> or
<a href="http://en.wikipedia.org/wiki/Logatome">logatomes</a>, similar to those found in
<a href="http://en.wikipedia.org/wiki/Jabberwocky">The Jabberwocky</a>
by <a href="http://en.wikipedia.org/wiki/Lewis_Carroll">Lewis Carroll</a>, using three
probabilistic models of word creation that become progressively more accurate.

<h2>Naive Design</h2>
<p>As a first approach, we can start by selecting
characters from the alphabet uniformly at random.  However, we will
randomly choose the lengths of our nonsense words according to the
distribution of real English words, illustrated in the figure below.
Of course, this is not a uniform distribution&mdash;there are many words
with around 7&ndash;10 letters, and the number of words decreases as the
words get either shorter or longer.</p>

<p>First, write a function called <code>word_lengths</code> to calculate a
histogram of word lengths based on a comprehensive list of words in
the English language. This function should read the list of words
from <a href="../data/english2.txt">english2.txt</a>, and return a
dictionary with integer lengths as keys and the frequency counts
  (i.e. <code>float</code> values between 0 and 1) as values.</p>

<p>
<center><img width=600 src="../images/wordlength.png"></center>
<p>
Next, write a second function called <code>proportional_choice</code>
that takes a histogram as a parameter (like the one returned
by <code>word_lengths</code>) and returns one of the dictionary keys,
selected at random with probability proportional to the frequency. For
example, if words of length 4 comprised 17% of the word list, your
function should return 4 on average 17% of the time.  Be sure that
your function only returns keys which are actually in the dictionary!
For example, <code>proportional_choice({ 3 : 0.5, 6 : 0.5 })</code>
should return 3 half the time on average and 6 the other half of the
time.
<p>
Finally, write a function called <code>nonsense</code> that takes as a
parameter a count of how many words to generate. It should first
calculate the word length frequency dictionary
using <code>word_lengths</code>.  Then, for each word, randomly select
a length using <code>proportional_choice</code>, and uniformly select
random characters for this word.  Finally, it should return a string
with the generated words, separated by spaces.  For
example, <code>nonsense(5)</code> might return <code>'gcennufgr
    wmpclprgv nfsmlms bhilnqtugxz sbutwswctyk'</code>.

<h2>Letter Frequency</h2>

The words generated naively above are not really pseudowords because
they tend to be unpronounceable. Our second approach will be to change
the letter selection from uniform to proportional as well. Recall that
the frequency of letters in English is not uniform: E is the most
frequent letter, followed by, T, A, O, I, N...  Write a function
called <code>letter_freqs</code> which will calculate a histogram
dictionary with letters as keys and letter frequency counts as values.
<p>
<center><img width=600 src="../images/English-slf.png"></center>
<p>
Revise your naive <code>nonsense</code> function above to use the
proportional probability function twice&mdash;once for length, and
then once for letters&mdash;to generate the words.  For
example, <code>nonsense(5)</code> might now
return <code>'ilaregicmsaedlmel sesiyeh irrnsclanr iinstosrrom
dsilzigcsieiird'</code>.

<h2>Markov Chains</h2>

We are getting closer to pronounceability, but to really make the
right kind of words, we need to care about the arrangement of the
letters along with their frequency. For example, the letter T is much
more likely to be followed by an H than by a Z. To do this, we create
a
<a href="http://en.wikipedia.org/wiki/Markov_chain">Markov chain</a> to generate the next letter in a
sequence based on the previous letters.
<p>
<center><img width=400 src="../images/26vertex.png"></center>
<p>
We start with a Markov chain of order 1. The order of the Markov chain
is how many of these previous letters to use. This generates the graph
above, where each letter is connected to every other letter, including
itself, and the edges are the probability of choosing the connected
letter next. If we also include "\n" as a possible character, our
words will naturally terminate.
<p>
Write a function <code>markov</code> that takes two parameters, a list
of words and an <code>int</code> value called <code>n</code>
representing the order.  This function will create a dictionary whose
keys are strings of <code>n</code> letters, and whose values are more
dictionaries.  These inner dictionaries are similar to the previous
letter frequency dictionaries and can be used in the same way to
generate the next letter. For example, for an order-1 Markov chain XXX

Revise your function above to use this dictionary to generate
words. When you generate a <code>"\n"</code> character, start with the next word.
<p>
<center><img width=400 src="../images/YO-DAWG.jpg"></center>
<p>
Experiment with your function to find the order which creates the best
nonsense words. What happens when the order is very large?

<h2>Spells</h2>
As a final task, substitute the English word list with Latin to generate nonsense
spells, such as "patuscus immoveo" or "stulor oportus."
<p>
TRY TO CREATE TEXTS FROM NOVELS
<p>
WRITE DEFINITIONS FOR TWO OF YOUR RANDOM WORDS
<p>
ADD WHAT TO TURN IN
<p>
ADD GRADING SCHEME
<hr>
<small>&copy; Mark Goadrich and Brent Yorgey, Hendrix College</small>
</BODY>
</HTML>
